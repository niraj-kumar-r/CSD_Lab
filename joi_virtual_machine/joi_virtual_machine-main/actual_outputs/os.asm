.section
.data
.section
.text
jal x30,__joi
__OS_mread:
sw x1,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
sw x2,0(x5)
addi x6,x2,40
lui x5,2
addi x5,x5,12
sw x2,0(x5)
addi x2,x2,60
lui x5,2
addi x5,x5,8
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
# Access value at address
addi x2,x2,-4
lw x21,0(x2)
lw x20,0(x21)
sw x20,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,8
lw x6,0(x6)
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x2,0(x5)
lw x5,-8(x2)
lui x6,2
addi x6,x6,16
sw x5,0(x6)
lw x5,-12(x2)
lui x6,2
addi x6,x6,12
sw x5,0(x6)
lw x7,-16(x2)
lw x5,-20(x2)
lui x6,2
addi x6,x6,4
sw x5,0(x6)
lw x5,-4(x2)
lui x6,2
addi x6,x6,8
lw x2,0(x6)
addi x2,x2,4
lui x6,2
addi x6,x6,8
sw x7,0(x6)
jalr x0,x1,0
__OS_mwrite:
sw x1,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
sw x2,0(x5)
addi x6,x2,40
lui x5,2
addi x5,x5,12
sw x2,0(x5)
addi x2,x2,60
lui x5,2
addi x5,x5,8
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,16
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
# Store value at address
addi x2,x2,-4
lw x20,0(x2)
addi x2,x2,-4
lw x21,0(x2)
sw x20,0(x21)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,8
lw x6,0(x6)
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x2,0(x5)
lw x5,-8(x2)
lui x6,2
addi x6,x6,16
sw x5,0(x6)
lw x5,-12(x2)
lui x6,2
addi x6,x6,12
sw x5,0(x6)
lw x7,-16(x2)
lw x5,-20(x2)
lui x6,2
addi x6,x6,4
sw x5,0(x6)
lw x5,-4(x2)
lui x6,2
addi x6,x6,8
lw x2,0(x6)
addi x2,x2,4
lui x6,2
addi x6,x6,8
sw x7,0(x6)
jalr x0,x1,0
addi x5,x0,64
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,37
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,12
sw x5,0(x6)
addi x5,x0,10
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,16
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,20
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,24
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,28
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,32
sw x5,0(x6)
__write_char_at:
sw x1,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
sw x2,0(x5)
addi x6,x2,40
lui x5,2
addi x5,x5,12
sw x2,0(x5)
addi x2,x2,60
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,8
lw x6,0(x6)
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x2,0(x5)
lw x5,-8(x2)
lui x6,2
addi x6,x6,16
sw x5,0(x6)
lw x5,-12(x2)
lui x6,2
addi x6,x6,12
sw x5,0(x6)
lw x7,-16(x2)
lw x5,-20(x2)
lui x6,2
addi x6,x6,4
sw x5,0(x6)
lw x5,-4(x2)
lui x6,2
addi x6,x6,8
lw x2,0(x6)
addi x2,x2,4
lui x6,2
addi x6,x6,8
sw x7,0(x6)
jalr x0,x1,0
__save_screen:
sw x1,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
sw x2,0(x5)
addi x6,x2,40
lui x5,2
addi x5,x5,12
sw x2,0(x5)
addi x2,x2,60
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,8
lw x6,0(x6)
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x2,0(x5)
lw x5,-8(x2)
lui x6,2
addi x6,x6,16
sw x5,0(x6)
lw x5,-12(x2)
lui x6,2
addi x6,x6,12
sw x5,0(x6)
lw x7,-16(x2)
lw x5,-20(x2)
lui x6,2
addi x6,x6,4
sw x5,0(x6)
lw x5,-4(x2)
lui x6,2
addi x6,x6,8
lw x2,0(x6)
addi x2,x2,4
lui x6,2
addi x6,x6,8
sw x7,0(x6)
jalr x0,x1,0
__write_int:
sw x1,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
sw x2,0(x5)
addi x6,x2,40
lui x5,2
addi x5,x5,12
sw x2,0(x5)
addi x2,x2,60
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,8
lw x6,0(x6)
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x2,0(x5)
lw x5,-8(x2)
lui x6,2
addi x6,x6,16
sw x5,0(x6)
lw x5,-12(x2)
lui x6,2
addi x6,x6,12
sw x5,0(x6)
lw x7,-16(x2)
lw x5,-20(x2)
lui x6,2
addi x6,x6,4
sw x5,0(x6)
lw x5,-4(x2)
lui x6,2
addi x6,x6,8
lw x2,0(x6)
addi x2,x2,4
lui x6,2
addi x6,x6,8
sw x7,0(x6)
jalr x0,x1,0
lui x5,61
addi x5,x5,148
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,12
sw x5,0(x6)
addi x5,x0,1024
sw x5,0(x2)
addi x2,x2,4
# Storing pointer information
lui x5,2
addi x5,x5,24
add x5,x5,x8
lw x6,0(x5)
lui x7,3
addi x7,x7,-1984
sw x7,0(x6)
addi x7,x0,1024
sw x7,4(x6)
addi x7,x0,1
sw x7,8(x6)
addi x6,x6,12
sw x6,0(x5)
# Pushing pointer triplet to stack
lui x5,3
addi x5,x5,-1984
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1024
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,60
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,60
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,248
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,118
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,220
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,118
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,60
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,60
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,6
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,128
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,206
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,222
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,246
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,230
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,112
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,28
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,60
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,30
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,248
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,248
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,112
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,222
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,222
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,222
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,60
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,60
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,248
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,248
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,98
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,104
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,104
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,98
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,98
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,104
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,104
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,240
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,60
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,206
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,62
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,30
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,230
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,230
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,240
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,98
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,238
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,214
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,230
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,246
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,222
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,206
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,240
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,220
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,28
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,230
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,224
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,112
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,28
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,180
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,214
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,238
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,140
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,50
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,6
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,2
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,16
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,118
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,224
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,220
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,28
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,118
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,240
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,240
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,118
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,248
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,224
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,118
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,230
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,112
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,224
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,230
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,112
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,214
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,248
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,220
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,240
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,118
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,30
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,220
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,118
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,102
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,96
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,240
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,192
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,248
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,16
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,52
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,118
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,120
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,214
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,254
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,56
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,108
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,198
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,204
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,124
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,248
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,152
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,100
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,252
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,28
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,224
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,28
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,24
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,224
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,28
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,48
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,224
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,118
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,220
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,24
sw x5,0(x6)
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,20
sw x5,0(x6)
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,16
sw x5,0(x6)
__clear_screen:
sw x1,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
sw x2,0(x5)
addi x6,x2,40
lui x5,2
addi x5,x5,12
sw x2,0(x5)
addi x2,x2,60
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_if_0_no
jal x30,__end_if_0
__end_if_0_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,1
addi x5,x5,703
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,12
sw x5,0(x6)
__L0:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__#end_L0_no
jal x30,__#end_L0
__#end_L0_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
lw x7,0(x5)
addi x5,x2,-12
lui x6,2
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
sw x7,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,12
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,16
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
jal x1,__OS_mwrite
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
jal x30,__L0
__end_L0:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
jal x30,__end_if_0
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,8
lw x6,0(x6)
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x2,0(x5)
lw x5,-8(x2)
lui x6,2
addi x6,x6,16
sw x5,0(x6)
lw x5,-12(x2)
lui x6,2
addi x6,x6,12
sw x5,0(x6)
lw x7,-16(x2)
lw x5,-20(x2)
lui x6,2
addi x6,x6,4
sw x5,0(x6)
lw x5,-4(x2)
lui x6,2
addi x6,x6,8
lw x2,0(x6)
addi x2,x2,4
lui x6,2
addi x6,x6,8
sw x7,0(x6)
jalr x0,x1,0
__write_char:
sw x1,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
sw x2,0(x5)
addi x6,x2,40
lui x5,2
addi x5,x5,12
sw x2,0(x5)
addi x2,x2,60
lui x5,2
addi x5,x5,8
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_if_1_no
jal x30,__end_if_1
__end_if_1_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,10
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__elseif_0_no
jal x30,__elseif_0
__elseif_0_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_0_loop:
            beq x29,x0,__mul_0_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_0_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_0_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_0_loop
            
            __mul_0_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_if_3_no
jal x30,__end_if_3
__end_if_3_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
lw x7,0(x5)
addi x5,x2,-4
lui x6,2
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
sw x7,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,12
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,16
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
jal x1,__clear_screen
jal x30,__end_if_3
jal x30,__end_if_2
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,95
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__elseif_1_no
jal x30,__elseif_1
__elseif_1_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_if_4_no
jal x30,__end_if_4
__end_if_4_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_1_loop:
            beq x29,x0,__mul_1_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_1_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_1_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_1_loop
            
            __mul_1_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_if_5_no
jal x30,__end_if_5
__end_if_5_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
jal x30,__end_if_5
jal x30,__end_if_4
jal x30,__end_if_2
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,13
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__elseif_2_no
jal x30,__elseif_2
__elseif_2_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
jal x30,__end_if_2
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,12
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__elseif_3_no
jal x30,__elseif_3
__elseif_3_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_2_loop:
            beq x29,x0,__mul_2_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_2_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_2_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_2_loop
            
            __mul_2_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_if_6_no
jal x30,__end_if_6
__end_if_6_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
jal x30,__end_if_6
jal x30,__end_if_2
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,9
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__elseif_4_no
jal x30,__elseif_4
__elseif_4_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_if_7_no
jal x30,__end_if_7
__end_if_7_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_3_loop:
            beq x29,x0,__mul_3_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_3_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_3_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_3_loop
            
            __mul_3_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_if_8_no
jal x30,__end_if_8
__end_if_8_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
jal x30,__end_if_8
jal x30,__end_if_7
jal x30,__end_if_2
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__else_0_no
jal x30,__else_0
__else_0_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__else_1_no
jal x30,__else_1
__else_1_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__else_2_no
jal x30,__else_2
__else_2_no:
jal x30,__end_if_10
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
jal x30,__end_if_9
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_4_loop:
            beq x29,x0,__mul_4_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_4_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_4_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_4_loop
            
            __mul_4_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
div x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
div x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,12
sw x5,0(x6)
__L1:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_#L1_no
jal x30,__end_#L1
__end_#L1_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
div x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_5_loop:
            beq x29,x0,__mul_5_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_5_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_5_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_5_loop
            
            __mul_5_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
lw x7,0(x5)
addi x5,x2,-8
lui x6,2
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
sw x7,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,12
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,16
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
jal x1,__OS_mread
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,256
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
div x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
rem x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,12
sw x5,0(x6)
addi x5,x0,3
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,16
sw x5,0(x6)
__L2:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,16
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_#L2_no
jal x30,__end_#L2
__end_#L2_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,256
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
div x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,16
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,16
sw x5,0(x6)
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
jal x30,__L2
__end___L2:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
div x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_6_loop:
            beq x29,x0,__mul_6_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_6_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_6_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_6_loop
            
            __mul_6_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
lw x7,0(x5)
addi x5,x2,-12
lui x6,2
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
sw x7,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,12
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,16
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
jal x1,__OS_mwrite
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
jal x30,__L1
__end___L1:
jal x30,__end_if_2
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
# Storing pointer information
lui x5,2
addi x5,x5,24
add x5,x5,x8
lw x6,0(x5)
lui x7,4
addi x7,x7,-1984
sw x7,0(x6)
addi x7,x0,8
sw x7,4(x6)
addi x7,x0,1
sw x7,8(x6)
addi x6,x6,12
sw x6,0(x5)
# Pushing pointer triplet to stack
lui x5,4
addi x5,x5,-1984
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,12
sw x5,0(x6)
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
__L3:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_#L3_no
jal x30,__end_#L3
__end_#L3_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
# Calculate array index address
addi x2,x2,-4
lw x20,0(x2)
addi x2,x2,-12
lw x21,0(x2)
lw x22,4(x2)
lw x23,8(x2)
bge x20,x22,__array_out_of_bounds
blt x20,x0,__array_out_of_bounds
# Determine element size based on type
addi x24,x0,0
jal x1,__type_check
            # Multiplication of x20 and x24
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x20,x0  # Copy multiplicand
            add x29,x24,x0  # Copy multiplier
            
            __mul_7_loop:
            beq x29,x0,__mul_7_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_7_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_7_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_7_loop
            
            __mul_7_done:
            add x20,x26,x0   # Move result to destination
        add x20,x20,x21
sw x20,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,16
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,128
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_8_loop:
            beq x29,x0,__mul_8_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_8_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_8_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_8_loop
            
            __mul_8_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
# Calculate array index address
addi x2,x2,-4
lw x20,0(x2)
addi x2,x2,-12
lw x21,0(x2)
lw x22,4(x2)
lw x23,8(x2)
bge x20,x22,__array_out_of_bounds
blt x20,x0,__array_out_of_bounds
# Determine element size based on type
addi x24,x0,0
jal x1,__type_check
            # Multiplication of x20 and x24
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x20,x0  # Copy multiplicand
            add x29,x24,x0  # Copy multiplier
            
            __mul_9_loop:
            beq x29,x0,__mul_9_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_9_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_9_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_9_loop
            
            __mul_9_done:
            add x20,x26,x0   # Move result to destination
        add x20,x20,x21
sw x20,0(x2)
addi x2,x2,4
# Access value at address
addi x2,x2,-4
lw x21,0(x2)
lw x20,0(x21)
sw x20,0(x2)
addi x2,x2,4
# Store value at address
addi x2,x2,-4
lw x20,0(x2)
addi x2,x2,-4
lw x21,0(x2)
sw x20,0(x21)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
jal x30,__L3
__end___L3:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_10_loop:
            beq x29,x0,__mul_10_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_10_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_10_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_10_loop
            
            __mul_10_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
div x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
div x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,12
sw x5,0(x6)
__L4:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_#L4_no
jal x30,__end_#L4
__end_#L4_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
div x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_11_loop:
            beq x29,x0,__mul_11_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_11_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_11_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_11_loop
            
            __mul_11_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
lw x7,0(x5)
addi x5,x2,-8
lui x6,2
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
sw x7,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,12
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,16
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
jal x1,__OS_mread
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
# Calculate array index address
addi x2,x2,-4
lw x20,0(x2)
addi x2,x2,-12
lw x21,0(x2)
lw x22,4(x2)
lw x23,8(x2)
bge x20,x22,__array_out_of_bounds
blt x20,x0,__array_out_of_bounds
# Determine element size based on type
addi x24,x0,0
jal x1,__type_check
            # Multiplication of x20 and x24
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x20,x0  # Copy multiplicand
            add x29,x24,x0  # Copy multiplier
            
            __mul_12_loop:
            beq x29,x0,__mul_12_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_12_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_12_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_12_loop
            
            __mul_12_done:
            add x20,x26,x0   # Move result to destination
        add x20,x20,x21
sw x20,0(x2)
addi x2,x2,4
# Access value at address
addi x2,x2,-4
lw x21,0(x2)
lw x20,0(x21)
sw x20,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
rem x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,12
sw x5,0(x6)
addi x5,x0,3
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,16
sw x5,0(x6)
__L5:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,16
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,12
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_#L5_no
jal x30,__end_#L5
__end_#L5_no:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,256
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_13_loop:
            beq x29,x0,__mul_13_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_13_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_13_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_13_loop
            
            __mul_13_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,16
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,16
sw x5,0(x6)
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
jal x30,__L5
__end___L5:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,4
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
div x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_14_loop:
            beq x29,x0,__mul_14_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_14_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_14_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_14_loop
            
            __mul_14_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
lw x7,0(x5)
addi x5,x2,-12
lui x6,2
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
sw x7,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,12
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,16
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
jal x1,__OS_mwrite
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
jal x30,__L4
__end___L4:
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
sub x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_if_11_no
jal x30,__end_if_11
__end_if_11_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
add x5,x6,x5
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
jal x30,__end_if_11
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,4
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x5,x0,8
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,4
add x5,x5,x8
lw x5,0(x5)
addi x5,x5,8
lw x5,0(x5)
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x2,x2,-4
lw x6,0(x2)
            # Multiplication of x6 and x5
            addi x26,x0,0     # Initialize result
            addi x27,x0,0     # Initialize counter
            add x28,x6,x0  # Copy multiplicand
            add x29,x5,x0  # Copy multiplier
            
            __mul_15_loop:
            beq x29,x0,__mul_15_done
            andi x30,x29,1    # Check LSB
            beq x30,x0,__mul_15_shift
            add x26,x26,x28   # Add multiplicand to result
            
            __mul_15_shift:
            slli x28,x28,1    # Shift multiplicand left
            srli x29,x29,1    # Shift multiplier right
            bge x0,x0,__mul_15_loop
            
            __mul_15_done:
            add x5,x26,x0   # Move result to destination
        sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
addi x6,x0,1
beq x5,x6,__end_if_12_no
jal x30,__end_if_12
__end_if_12_no:
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,4
sw x5,0(x6)
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,4
add x6,x6,x8
lw x6,0(x6)
addi x6,x6,8
sw x5,0(x6)
jal x30,__end_if_12
jal x30,__end_if_1
addi x5,x0,1
sw x5,0(x2)
addi x2,x2,4
addi x2,x2,-4
lw x5,0(x2)
lui x6,2
addi x6,x6,8
lw x6,0(x6)
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x2,0(x5)
lw x5,-8(x2)
lui x6,2
addi x6,x6,16
sw x5,0(x6)
lw x5,-12(x2)
lui x6,2
addi x6,x6,12
sw x5,0(x6)
lw x7,-16(x2)
lw x5,-20(x2)
lui x6,2
addi x6,x6,4
sw x5,0(x6)
lw x5,-4(x2)
lui x6,2
addi x6,x6,8
lw x2,0(x6)
addi x2,x2,4
lui x6,2
addi x6,x6,8
sw x7,0(x6)
jalr x0,x1,0
__joi:
lui x5,2
addi x5,x5,32
lui x6,2
addi x6,x6,4
add x6,x8,x6
sw x5,0(x6)
lui x5,2
addi x5,x5,544
lui x6,2
addi x6,x6,8
add x6,x8,x6
sw x5,0(x6)
lui x5,2
addi x5,x5,576
lui x6,2
addi x6,x6,12
add x6,x8,x6
sw x5,0(x6)
lui x5,2
addi x5,x5,1088
lui x6,2
add x6,x8,x6
sw x5,0(x6)
lui x5,3
addi x5,x5,-1984
lui x6,2
addi x6,x6,16
add x6,x8,x6
sw x5,0(x6)
lui x5,5
addi x5,x5,-176
lui x6,2
addi x6,x6,24
add x6,x8,x6
sw x5,0(x6)
lui x5,64
lui x6,2
addi x6,x6,20
add x6,x8,x6
sw x5,0(x6)
lui x2,2
addi x2,x2,1088
add x2,x2,x8
addi x5,x0,72
sw x5,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,8
lw x7,0(x5)
addi x5,x2,-8
lui x6,2
addi x6,x6,8
sw x5,0(x6)
lui x5,2
addi x5,x5,4
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
sw x7,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,12
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
lui x5,2
addi x5,x5,16
lw x6,0(x5)
sw x6,0(x2)
addi x2,x2,4
jal x1,__write_char
addi x5,x0,0
sw x5,0(x2)
addi x2,x2,4
jal x30,__END__
jal x30,__END__
__type_check:
addi x25,x0,1
beq x23,x25,__type_int
addi x25,x0,2
beq x23,x25,__type_float
addi x25,x0,3
beq x23,x25,__type_char
addi x25,x0,4
beq x23,x25,__type_bool
__type_int:
addi x24,x0,4
jalr x0,x1,0
__type_float:
addi x24,x0,4
jalr x0,x1,0
__type_char:
addi x24,x0,1
jalr x0,x1,0
__type_bool:
addi x24,x0,1
jalr x0,x1,0
__array_out_of_bounds:
nop
__END__:
nop
